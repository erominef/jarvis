# ── Ollama — Primary inference server ────────────────────────────────────────
# The agent connects to Ollama via HTTP. Set to wherever your Ollama is running.
# If running in Docker with network_mode:host, this is literally http://localhost:11434
OLLAMA_PC_URL=http://localhost:11434

# ── Ollama — Heavy reasoning server (optional) ────────────────────────────────
# Second Ollama instance for large models. Can be same host or remote.
# If not set, all routing falls back to OLLAMA_PC_URL.
OLLAMA_XEON_URL=http://<OLLAMA_HOST>:11434

# ── Telegram ──────────────────────────────────────────────────────────────────
# Get bot token from @BotFather on Telegram.
TELEGRAM_BOT_TOKEN=0000000000:REPLACE_WITH_YOUR_TOKEN

# Your Telegram user ID (integer). The bot will only respond to this user.
# Find yours by messaging @userinfobot on Telegram.
TELEGRAM_OWNER_ID=000000000  # your numeric Telegram user id

# ── MCP / Knowledge Base (optional) ──────────────────────────────────────────
# If you have an MCP-compatible knowledge base server, configure it here.
# The client expects SSE JSON-RPC protocol (MCP spec 2024-11-05).
MCP_MONTEGALLO_URL=https://your-mcp-server.example.com/sse
MCP_MONTEGALLO_TOKEN=your_token_here

# ── Twilio / SMS (optional, not yet active) ───────────────────────────────────
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_PHONE_NUMBER=
TWILIO_WEBHOOK_PORT=3000
